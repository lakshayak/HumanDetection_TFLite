{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ModelsHumanDetection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFF0Akaa4dCj"
      },
      "source": [
        "# Obtaining the pretrained model and Converting to TF-Lite\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayi7TmUiNCeZ"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import cv2\n",
        "import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIN_hH3qz2CI"
      },
      "source": [
        "# Cloning the tensorflow models \n",
        "!git clone --depth 1 https://github.com/tensorflow/models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQhykgf2nrwD"
      },
      "source": [
        "# Install the Object Detection API\n",
        "%%bash\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-BfFY7EnsU3"
      },
      "source": [
        "# Downloading & extracting the inbuilt SSD-Mobilenet V2 model\n",
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz\n",
        "!tar -xvf ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61ht2ncsnvF_"
      },
      "source": [
        "# Generates an intermediate SavedModel that can be used with the TFLite Converter\n",
        "!python /content/models/research/object_detection/export_tflite_graph_tf2.py \\\n",
        "    --trained_checkpoint_dir ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint \\\n",
        "    --pipeline_config_path ssd_mobilenet_v2_320x320_coco17_tpu-8/pipeline.config \\\n",
        "    --output_directory `pwd`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6F1vbXjs_X_"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFmbeD_9s8BJ"
      },
      "source": [
        "**CONVERSION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMkgiJUst0vk"
      },
      "source": [
        "dir = '/content/saved_model'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQG5Nnggt178"
      },
      "source": [
        "1) Without Any Quantisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyAxpyQK_XhS"
      },
      "source": [
        "# Convert the model to TF lite\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(dir)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Serialize the model\n",
        "open('tflite_model_nq.tflite', 'wb').write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_81REP3kG8q"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-PCki12tBgp"
      },
      "source": [
        "2) Dynamic Range Quantisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dba8WPiAgXNu"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(dir)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model_dr = converter.convert()\n",
        "open('tflite_model_dr.tflite', 'wb').write(tflite_model_dr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmNRWg91kJVl"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rcpUIiMtEXf"
      },
      "source": [
        "3) Full Integer Quantisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39mY8v-9d0Pp"
      },
      "source": [
        "def rep_data_gen():\n",
        "  a = []\n",
        "  for img in glob.glob(\"/content/sample/*.jpg\"):\n",
        "    data = cv2.imread(img)\n",
        "    img = cv2.resize(data, (300, 300))\n",
        "    img = img / 255.0\n",
        "    img = img.astype(np.float32)\n",
        "    a.append(img)\n",
        "    a1 = np.array(a)\n",
        "    img = tf.data.Dataset.from_tensor_slices(a1).batch(1)\n",
        "    for i in img:\n",
        "        yield [i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHchOLmo511t"
      },
      "source": [
        "# In onder to do full integer quantization, a range should be estimated of all the floating point tensors in the model\n",
        "# For variable tensors, the convertor needs a representative dataset with a few inference cycles to calibrate\n",
        "# Loop over every image and perform detection\n",
        "converter_int = tf.lite.TFLiteConverter.from_saved_model(dir)\n",
        "converter_int.representative_dataset = rep_data_gen\n",
        "converter_int.optimizations = [tf.lite.Optimize.DEFAULT] # Using the default optimisation strategy\n",
        "converter_int.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] # Ensures that TFlite interpreter only uses integers\n",
        "converter_int.inference_input_type = tf.uint8  # Setting input type to be int8\n",
        "converter_int.inference_output_type = tf.uint8  # Setting output type to be int8\n",
        "tflite_quant_model_int = converter_int.convert() # Converts a saved model into TensorFlow Lite model\n",
        "open('tflite_quant_model_int.tflite', 'wb').write(tflite_quant_model_int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFatIEyckYet"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ueU6gNMeQfX"
      },
      "source": [
        "4) Float-Fallback Quantisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p0_cpupjfmG"
      },
      "source": [
        "# Path to the directory of the saved model\n",
        "converter_ff = tf.lite.TFLiteConverter.from_saved_model(dir)\n",
        "# Assigning the representative dataset generator \n",
        "converter_ff.representative_dataset = rep_data_gen \n",
        "# Using the default optimisation strategy\n",
        "converter_ff.optimizations = [tf.lite.Optimize.DEFAULT] \n",
        "\n",
        "# Converts a saved model into TensorFlow Lite model\n",
        "tflite_quant_model_ff = converter_ff.convert() \n",
        "open('tflite_quant_model_ff.tflite', 'wb').write(tflite_quant_model_ff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i6cVjx8kZyn"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwDbrvl2euDf"
      },
      "source": [
        "5) Float16 Quantisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyH0hSZ1dk99"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(dir)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "tflite_quant_model_f16 = converter.convert()\n",
        "open('tflite_quant_model_f16.tflite', 'wb').write(tflite_quant_model_f16)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}